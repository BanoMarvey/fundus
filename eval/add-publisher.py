import argparse
import gzip
import json
from collections import namedtuple
from pathlib import Path

from fundus import Crawler, PublisherCollection
from fundus import __development_base_path__ as root_path

Namespace = namedtuple("Namespace", ("publisher", "number_of_articles"))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="autogenerated html/json entries to simplify automation",
        description=(
            "script to automatically scrawl <n> (default: 5) articles, save the HTML files under eval/html and add "
            "auto-parsed paragraphs to ground_truth.json. this script will wrapp every paragraph with "
            "<auto> tags so that no autogenerated test case is added without human error checking. care, "
            "this will overwrite existing files"
        ),
    )
    parser.add_argument("publisher", metavar="P", type=str, help="publisher to add to evaluation")
    parser.add_argument(
        "-n",
        dest="number_of_articles",
        metavar="Number",
        type=int,
        default=5,
        help="number of articles to add to the evaluation",
    )
    parser.add_argument(
        "-p",
        dest="eval_dir",
        metavar="Path",
        type=str,
        help="path to evaluation dictionary; default is <root-dir>/eval",
    )

    args = parser.parse_args()

    publisher = PublisherCollection[args.publisher]  # type: ignore

    crawler = Crawler(publisher)
    publisher_mapping = {publisher.publisher_name: publisher.name}

    eval_dir = Path(args.eval_dir) if args.eval_dir is not None else root_path / "eval"
    html_dir = eval_dir / "html"
    ground_truth_file_path = eval_dir / "ground_truth.json"
    html_dir.mkdir(parents=True, exist_ok=True)

    if not ground_truth_file_path.exists():
        ground_truth = {}
    else:
        with open(ground_truth_file_path, "r", encoding="utf-8") as ground_truth_file:
            ground_truth = json.load(ground_truth_file)

    for idx, article in enumerate(crawler.crawl(max_articles=args.number_of_articles)):
        file_name = html_dir / f"{publisher_mapping[article.html.source.publisher]}_{idx}.html.gz"
        with open(file_name, "wb") as html_file:
            html_file.write(gzip.compress(bytes(article.html.content, "utf-8")))

        assert article.body
        paragraphs = [f"<auto>{paragraph}<auto>" for paragraph in article.body.as_text_sequence()]

        entry = {
            "body": paragraphs,
            "url": article.html.responded_url,
            "crawl_date": str(article.html.crawl_date),
        }

        ground_truth[file_name.name] = entry

    with open(ground_truth_file_path, "w", encoding="utf-8") as ground_truth_file:
        test = json.dumps(ground_truth, indent=4, ensure_ascii=False)
        ground_truth_file.write(test)
